{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1o6orxhxIapGZWt5FD4v7T8BRwdcSCEW4","timestamp":1684744697778},{"file_id":"1dpY2_Um7w8Qqx5c0e_vD2qaXXFW2LE3j","timestamp":1684248464934}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"kaht-FPA1Jvq"},"cell_type":"markdown","source":["# Introduction\n","\n","## Lab2: Train a Convolutional Neural Network (CNN).\n","\n","In this Lab session we will learn how to train a CNN from scratch for classifying MNIST digits."]},{"metadata":{"id":"UvxtTYHlVfRK"},"cell_type":"code","source":["# import necessary libraries\n","import torch\n","import torchvision\n","from torchvision import transforms as T\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"metadata":{"id":"HYCvhGxKWyN7"},"cell_type":"markdown","source":["### Define LeNet\n","\n","![network architecture](https://www.researchgate.net/profile/Lucijano-Berus/publication/329891470/figure/fig1/AS:707347647307776@1545656229128/Architecture-of-LeNet-5-a-Convolutional-Neural-Network-for-digits-digits-recognition-An.ppm)\n","\n","Here we are going to define our first CNN which is **LeNet** in this case. This architecture has been introduced and is detailed in [this article](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf). To construct a LeNet we will be using some convolutional layers followed by some fully-connected layers. The convolutional layers can be simply defined using `torch.nn.Conv2d` module of `torch.nn` package. Details can be found [here](https://pytorch.org/docs/stable/nn.html#conv2d). Moreover, we will use pooling operation to reduce the size of convolutional feature maps. For this case we are going to use `torch.nn.functional.max_pool2d`. Details about maxpooling can be found [here](https://pytorch.org/docs/stable/nn.html#max-pool2d)\n","\n","Differently from our previous Lab, we will use a Rectified Linear Units (ReLU) as activation function with the help of `torch.nn.functional.relu`, replacing `torch.nn.Sigmoid`. Details about ReLU can be found [here](https://pytorch.org/docs/stable/nn.html#id26)."]},{"metadata":{"id":"dMC_LDYdWkI7"},"cell_type":"code","source":["class LeNet(torch.nn.Module):\n","  def __init__(self):\n","    super(LeNet, self).__init__()\n","\n","    # input channel = ?, output channels = ?, kernel size = ?\n","    # input image size = (?, ?), image output size = (?, ?)\n","    # TODO\n","\n","    # input channel = ?, output channels = ?, kernel size = ?\n","    # input image size = (?, ?), output image size = (?, ?)\n","    # TODO\n","\n","    # input dim = ? ( H x W x C), output dim = ?\n","    # TODO\n","\n","    # input dim = ?, output dim = ?\n","    # TODO\n","\n","    # input dim = ?, output dim = ?\n","    # TODO\n","\n","  def forward(self, x):\n","\n","    # TODO\n","    # Max Pooling with kernel size = ?\n","    # output size = (?, ?)\n","    # TODO\n","\n","    # TODO\n","    # Max Pooling with kernel size = ?\n","    # output size = (?, ?)\n","    # TODO\n","\n","    # flatten the feature maps into a long vector\n","    x = x.view(x.shape[0], -1)\n","\n","    # TODO\n","\n","    # TODO\n","\n","    # TODO\n","\n","    return x"],"execution_count":null,"outputs":[]},{"metadata":{"id":"gChf6TvWonrV"},"cell_type":"markdown","source":["### Define cost function"]},{"metadata":{"id":"6j5UrBH3oek8"},"cell_type":"code","source":["def get_cost_function():\n","  cost_function = #...\n","  return cost_function"],"execution_count":null,"outputs":[]},{"metadata":{"id":"U2TjXeVdorV9"},"cell_type":"markdown","source":["### Define the optimizer\n","\n","We will use SGD with learning rate-lr, weight_decay=wd and  momentum=momentum"]},{"metadata":{"id":"hBZN-WPboulR"},"cell_type":"code","source":["def get_optimizer(net, lr, wd, momentum):\n","  optimizer =  #..."],"execution_count":null,"outputs":[]},{"metadata":{"id":"wTkfrV64oxIL"},"cell_type":"markdown","source":["### Train and test functions"]},{"metadata":{"id":"t-sE5vFio0lf"},"cell_type":"code","source":["def test(net, data_loader, cost_function, device='cuda:0'):\n","  samples = 0.\n","  cumulative_loss = 0.\n","  cumulative_accuracy = 0.\n","\n","  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n","  with torch.no_grad():\n","    for batch_idx, (inputs, targets) in enumerate(data_loader):\n","      # Load data into GPU\n","      inputs = inputs.to(device)\n","      targets = targets.to(device)\n","\n","      # Forward pass\n","      outputs = net(inputs)\n","\n","      # Apply the loss\n","      loss = cost_function(outputs, targets)\n","\n","      # Better print something\n","      samples+=inputs.shape[0]\n","      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n","      _, predicted = outputs.max(1)\n","      cumulative_accuracy += predicted.eq(targets).sum().item()\n","\n","  return cumulative_loss/samples, cumulative_accuracy/samples*100\n","\n","\n","def train(net,data_loader,optimizer,cost_function, device='cuda:0'):\n","  samples = 0.\n","  cumulative_loss = 0.\n","  cumulative_accuracy = 0.\n","\n","\n","  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n","  for batch_idx, (inputs, targets) in enumerate(data_loader):\n","    # Load data into GPU\n","    inputs = inputs.to(device)\n","    targets = targets.to(device)\n","\n","    # Forward pass\n","    outputs = net(inputs)\n","\n","    # Apply the loss\n","    loss = cost_function(outputs,targets)\n","\n","    # Reset the optimizer\n","\n","    # Backward pass\n","    loss.backward()\n","\n","    # Update parameters\n","    optimizer.step()\n","\n","    optimizer.zero_grad()\n","\n","    # Better print something, no?\n","    samples+=inputs.shape[0]\n","    cumulative_loss += loss.item()\n","    _, predicted = outputs.max(1)\n","    cumulative_accuracy += predicted.eq(targets).sum().item()\n","\n","  return cumulative_loss/samples, cumulative_accuracy/samples*100"],"execution_count":null,"outputs":[]},{"metadata":{"id":"T6IT0Lsgo8AM"},"cell_type":"markdown","source":["### Define the function that fetches a data loader that is then used during iterative training.\n","\n","We will learn a new thing in this function as how to Normalize the inputs given to the network.\n","\n","***Why Normalization is needed***?\n","\n","To have nice and stable training of the network it is recommended to normalize the network inputs between \\[-1, 1\\].\n","\n","***How it can be done***?\n","\n","This can be simply done using `torchvision.transforms.Normalize()` transform. Details can be found [here](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize)."]},{"metadata":{"id":"qDxpo6uVo_8k"},"cell_type":"code","source":["def get_data(batch_size, test_batch_size=256):\n","\n","  # Prepare data transformations and then combine them sequentially\n","  transform = list()\n","  transform.append(T.ToTensor())                            # converts Numpy to Pytorch Tensor\n","  transform.append(T.Normalize(mean=[0.5], std=[0.5]))      # Normalizes the Tensors between [-1, 1]\n","  transform = T.Compose(transform)                          # Composes the above transformations into one.\n","\n","  # Load data\n","  full_training_data = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n","  test_data = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True)\n","\n","\n","  # Create train and validation splits\n","  num_samples = len(full_training_data)\n","  training_samples = int(num_samples*0.5+1)\n","  validation_samples = num_samples - training_samples\n","\n","  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n","\n","  # Initialize dataloaders\n","  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n","  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n","  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n","\n","  return train_loader, val_loader, test_loader"],"execution_count":null,"outputs":[]},{"metadata":{"id":"OHcB8f0AsY4n"},"cell_type":"markdown","source":["### Wrapping everything up\n","\n","Finally, we need a main function which initializes everything + the needed hyperparameters and loops over multiple epochs (printing the results)."]},{"metadata":{"id":"ip_R-hruse0Q"},"cell_type":"code","source":["'''\n","Input arguments\n","  batch_size: Size of a mini-batch\n","  device: GPU where you want to train your network\n","  weight_decay: Weight decay co-efficient for regularization of weights\n","  momentum: Momentum for SGD optimizer\n","  epochs: Number of epochs for training the network\n","'''\n","\n","def main(batch_size=128,\n","         device='cuda:0',\n","         learning_rate=0.01,\n","         weight_decay=0.000001,\n","         momentum=0.9,\n","         epochs=50):\n","\n","  train_loader, val_loader, test_loader = get_data(batch_size)\n","\n","  # TODO for defining LeNet-5\n","\n","  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n","\n","  cost_function = get_cost_function()\n","\n","  print('Before training:')\n","  train_loss, train_accuracy = test(net, train_loader, cost_function)\n","  val_loss, val_accuracy = test(net, val_loader, cost_function)\n","  test_loss, test_accuracy = test(net, test_loader, cost_function)\n","\n","  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n","  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n","  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n","  print('-----------------------------------------------------')\n","\n","  for e in range(epochs):\n","    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n","    val_loss, val_accuracy = test(net, val_loader, cost_function)\n","    print('Epoch: {:d}'.format(e+1))\n","    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n","    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n","    print('-----------------------------------------------------')\n","\n","  print('After training:')\n","  train_loss, train_accuracy = test(net, train_loader, cost_function)\n","  val_loss, val_accuracy = test(net, val_loader, cost_function)\n","  test_loss, test_accuracy = test(net, test_loader, cost_function)\n","\n","  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n","  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n","  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n","  print('-----------------------------------------------------')"],"execution_count":null,"outputs":[]},{"metadata":{"id":"ltdCMiB3t18h"},"cell_type":"markdown","source":["Lets train!"]},{"metadata":{"id":"6d-z20H4tziL"},"cell_type":"code","source":["main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using the proper metric from sklearn, check which character is most frequently confused with which: can you explain why ?\n","\n"],"metadata":{"id":"NQBDT48CKMVC"}},{"cell_type":"code","source":[],"metadata":{"id":"sm5b4MV1KzQ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The LeNet5 architecture can also be implemented using the sequential API ([see documentation ](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)). Reimplement it with this API."],"metadata":{"id":"ZOvixkfeMHrD"}},{"cell_type":"markdown","source":["##Experiments\n","\n","\n","* Implement adaptive early stopping: if the validation loss did not decrease for K consecutive epochs, stop training.\n","* Change dataset in order to evaluate the LeNet5 network on cifar10 dataset. You can have a look at the pytorch to easily access the cifar10 dataset.\n","* Try to improve performance with:\n","   *   data-augmentation\n","   *   dropout\n","* Implement the resnet18 architecture using the Resnet18 class from pytorch.\n","\n","\n","\n"],"metadata":{"id":"7i3-pC5xAyu5"}}]}